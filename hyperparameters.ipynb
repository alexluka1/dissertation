{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aYnXXhuhFNUW"
      },
      "outputs": [],
      "source": [
        "# Height, Width\n",
        "imageSize = (104,88)\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import Sequential\n",
        "from keras import layers\n",
        "\n",
        "import zipfile\n",
        "import os\n",
        "from skimage import io\n",
        "from skimage.transform import resize\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -q -U keras-tuner\n",
        "\n",
        "import keras_tuner as kt"
      ],
      "metadata": {
        "id": "whAbPt8QGd_y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Unzips file in path and extracts in to folder\n",
        "with zipfile.ZipFile('/content/data.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/data')"
      ],
      "metadata": {
        "id": "2CLDeDeoFTIy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Gets all the class names\n",
        "datasetPath = '/content/data/data/' # Change for data sets\n",
        "\n",
        "\n",
        "classes = os.listdir(datasetPath)\n",
        "# Adding all data to dataFiles\n",
        "dataFiles = np.empty(shape=(0,2))\n",
        "for c in classes:\n",
        "  # Creates the image file path in element [0] and the class of the image in element [1]\n",
        "  imgFile = np.array(list(map(lambda x: (f'{datasetPath}{c}/{x}', c), os.listdir(f'{datasetPath}{c}'))))\n",
        "  prevLength = len(dataFiles)\n",
        "  \n",
        "  dataFiles = np.concatenate((dataFiles, imgFile))\n",
        "  \n",
        "  print(f'{datasetPath}{c}: {len(dataFiles)} image files total. {len(dataFiles) - prevLength} images in class {c}')\n",
        "\n",
        "print(f'{dataFiles[0]}, {dataFiles[717]}, {dataFiles[769]}, {dataFiles[3329]}')\n",
        "\n",
        "dataSet = np.array(list(map(lambda x: (io.imread(x[0]), x[1]), dataFiles)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Heg-3LRjFUjD",
        "outputId": "8da76f0e-e7f5-44da-e1fa-160a9735cb9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/data/data/Non_Demented: 3200 image files total. 3200 images in class Non_Demented\n",
            "/content/data/data/Very_Mild_Demented: 5440 image files total. 2240 images in class Very_Mild_Demented\n",
            "/content/data/data/Moderate_Demented: 5504 image files total. 64 images in class Moderate_Demented\n",
            "/content/data/data/Mild_Demented: 6400 image files total. 896 images in class Mild_Demented\n",
            "['/content/data/data/Non_Demented/non_575.jpg' 'Non_Demented'], ['/content/data/data/Non_Demented/non_473.jpg' 'Non_Demented'], ['/content/data/data/Non_Demented/non_3044.jpg' 'Non_Demented'], ['/content/data/data/Very_Mild_Demented/verymild_1399.jpg'\n",
            " 'Very_Mild_Demented']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-15-5d4385b20b0e>:19: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  dataSet = np.array(list(map(lambda x: (io.imread(x[0]), x[1]), dataFiles)))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Making class dictionary to chnage class labels to numbers\n",
        "strDict = {\"Non_Demented\": 0, \"Very_Mild_Demented\": 1,   \"Mild_Demented\": 2, \"Moderate_Demented\": 3} # Change for data sets\n",
        "\n",
        "# dataSet is split into images and labels\n",
        "images = dataSet[:,0]\n",
        "labels = dataSet[:,1]\n",
        "\n",
        "\n",
        "print(labels[0:5])\n",
        "listLabels = list(labels)\n",
        "labels = np.array(list(map(lambda x: strDict[x], listLabels)))\n",
        "print(labels[0:5])\n",
        "\n",
        "#CHANGE#\n",
        "\n",
        "# Data is split into training and testing\n",
        "images_train, images_test, labels_train, labels_test = train_test_split(images, labels, test_size=0.33) # Take out random_state when testing without a fair test\n",
        "\n",
        "# Training data is split into validation data\n",
        "# images_train, images_val, labels_train, labels_val = train_test_split(images_train, labels_train, test_size=0.2, random_state=1)\n",
        "\n",
        "print(labels_train[0:4])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-uXtf8gpFcrx",
        "outputId": "54f7c631-3a39-414c-fc58-b694f450c79c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Non_Demented' 'Non_Demented' 'Non_Demented' 'Non_Demented'\n",
            " 'Non_Demented']\n",
            "[0 0 0 0 0]\n",
            "[1 0 1 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_images_train = []\n",
        "for im in images_train:\n",
        "  im = resize(im, imageSize, anti_aliasing=True)\n",
        "  new_images_train.append(im)\n",
        "images_train = new_images_train\n",
        "\n",
        "new_images_test = []\n",
        "for im in images_test:\n",
        "  im = resize(im, imageSize, anti_aliasing=True)\n",
        "  new_images_test.append(im)\n",
        "images_test = new_images_test\n",
        "\n",
        "#CHANGE#\n",
        "# new_images_val = []\n",
        "# for im in images_val:\n",
        "#   im = resize(im, imageSize, anti_aliasing=True)\n",
        "#   new_images_val.append(im)\n",
        "# images_val = new_images_val"
      ],
      "metadata": {
        "id": "tCxtNqYbFj-v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine Tuning Hyperparameters\n",
        "Build model and add points where tuner can tune model for improvement <br>\n",
        "Tunes: 3 filters, One dense layer, learning rate"
      ],
      "metadata": {
        "id": "jhnOFjj3G7cL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build model here\n",
        "def model_builder(hp):\n",
        "  # Deciding variabilites wanted in model\n",
        "  # Number of blocks\n",
        "  hp_blocks = hp.Int('blocks', min_value=1, max_value=4, step=1)\n",
        "\n",
        "  # Filters\n",
        "  hp_inp_filter = hp.Int('inp_filter', min_value=8, max_value=128, step=8)\n",
        "  hp_one_filter = hp.Int('inp_filter', min_value=8, max_value=128, step=8)\n",
        "  hp_block_filter = hp.Int('one_filter', min_value=8, max_value=256, step=8)\n",
        "  \n",
        "  # Kernel size?\n",
        "  hp_inp_kernel = hp.Int('inp_kernel', min_value=1, max_value=4, step=1)\n",
        "  hp_block_kernel = hp.Int('block_kernel', min_value=1, max_value=4, step=1)\n",
        "\n",
        "  # Max Pooling\n",
        "  hp_maxpooling = hp.Int('maxpooling', min_value=1, max_value=4, step=1)\n",
        "  hp_maxpooling_one = hp.Int('maxpooling_one', min_value=1, max_value=4, step=1)\n",
        "\n",
        "  # Dropout layers\n",
        "  hp_dropout = hp.Float('dropout', 0, 0.9, step=0.1)\n",
        "\n",
        "  # Number of nodes\n",
        "  hp_inp_dense_1 = hp.Int('dense1', min_value=8, max_value=1028, step=8)\n",
        "  hp_inp_dense_2 = hp.Int('dense2', min_value=8, max_value=1028, step=8)\n",
        "\n",
        "  # Tune the learning rate for the optimizer\n",
        "  hp_learning_rate = hp.Choice('learning_rate', values=[0.1, 0.01, 0.001, 0.0001])\n",
        "\n",
        "\n",
        "  # Model\n",
        "  def convBlock(model, filter, kernel, mpKernel): # Adds another convolutional layer\n",
        "    model.add(layers.MaxPooling2D(mpKernel))\n",
        "    model.add(layers.Conv2D(filter, kernel, activation='relu'))\n",
        "\n",
        "  model = keras.Sequential()\n",
        "  input_layer = layers.Conv2D(hp_inp_filter, (hp_inp_kernel,hp_inp_kernel), activation='relu', input_shape=(imageSize[0],imageSize[1],1))\n",
        "  input_layer._name = 'input' # Setting name of layer\n",
        "  model.add(input_layer)\n",
        "\n",
        "\n",
        "  model.add(layers.MaxPooling2D(hp_maxpooling_one))\n",
        "  model.add(layers.Conv2D(hp_one_filter, (1,1), activation='relu'))\n",
        "\n",
        "\n",
        "  for i in range(hp_blocks):\n",
        "    convBlock(model, hp_block_filter, (hp_block_kernel,hp_block_kernel), (hp_maxpooling,hp_maxpooling))\n",
        "\n",
        "  model.add(layers.Flatten())\n",
        "  model.add(layers.Dropout(hp_dropout))\n",
        "  model.add(layers.Dense(hp_inp_dense_1, activation='relu'))\n",
        "  # model.add(layers.Dropout(0.75))\n",
        "  # model.add(layers.Dense(128, activation='relu'))\n",
        "  model.add(layers.Dense(hp_inp_dense_2, activation='relu'))\n",
        "  model.add(layers.Dense(4, activation='softmax'))\n",
        "\n",
        "\n",
        "  # Compile\n",
        "  model.compile(optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate),\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "    metrics=['accuracy'])\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "7Aei7ZEzFlQE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tuner = kt.RandomSearch(model_builder,\n",
        "                    objective='val_accuracy',\n",
        "                    max_trials=50,\n",
        "                    executions_per_trial=1,\n",
        "                    directory='my_dirx'\n",
        "                     )\n",
        "\n",
        "\n",
        "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
        "\n"
      ],
      "metadata": {
        "id": "fZu9X7diGf3w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images_train = np.array(images_train)\n",
        "images_train = images_train.astype('float32')\n",
        "\n",
        "images_test = np.array(images_test)\n",
        "images_test = images_test.astype('float32') "
      ],
      "metadata": {
        "id": "QLaArWaKK90k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Get best learning rate, number of nodes in dense layer and convolution filter**"
      ],
      "metadata": {
        "id": "cU4SZuC0H0Mv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tuner.search(images_train, labels_train, epochs=50, validation_split=0.2, callbacks=[stop_early])\n",
        "\n",
        "# Get the optimal hyperparameters\n",
        "best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "\n",
        "print(best_hps)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "S3MVYpwKG4cB",
        "outputId": "443e73b4-e977-47d6-d5e7-e0b95c38f302"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 24 Complete [00h 00m 00s]\n",
            "\n",
            "Best val_accuracy So Far: 0.9731934666633606\n",
            "Total elapsed time: 00h 23m 05s\n",
            "\n",
            "Search: Running Trial #25\n",
            "\n",
            "Value             |Best Value So Far |Hyperparameter\n",
            "2                 |1                 |blocks\n",
            "40                |72                |inp_filter\n",
            "248               |176               |block_filter\n",
            "3                 |3                 |inp_kernel\n",
            "3                 |4                 |block_kernel\n",
            "3                 |2                 |maxpooling\n",
            "0                 |0.7               |dropout\n",
            "80                |480               |dense1\n",
            "928               |680               |dense2\n",
            "0.001             |0.001             |learning_rate\n",
            "216               |112               |one_filter\n",
            "3                 |2                 |maxpooling_one\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras_tuner/engine/base_tuner.py\", line 270, in _try_run_and_update_trial\n",
            "    self._run_and_update_trial(trial, *fit_args, **fit_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras_tuner/engine/base_tuner.py\", line 235, in _run_and_update_trial\n",
            "    results = self.run_trial(trial, *fit_args, **fit_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras_tuner/engine/tuner.py\", line 287, in run_trial\n",
            "    obj_value = self._build_and_fit_model(trial, *args, **copied_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras_tuner/engine/tuner.py\", line 213, in _build_and_fit_model\n",
            "    model = self._try_build(hp)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras_tuner/engine/tuner.py\", line 155, in _try_build\n",
            "    model = self._build_hypermodel(hp)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras_tuner/engine/tuner.py\", line 146, in _build_hypermodel\n",
            "    model = self.hypermodel.build(hp)\n",
            "  File \"<ipython-input-18-e06a5bd3019d>\", line 47, in model_builder\n",
            "    convBlock(model, hp_block_filter, (hp_block_kernel,hp_block_kernel), (hp_maxpooling,hp_maxpooling))\n",
            "  File \"<ipython-input-18-e06a5bd3019d>\", line 34, in convBlock\n",
            "    model.add(layers.Conv2D(filter, kernel, activation='relu'))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/trackable/base.py\", line 205, in _method_wrapper\n",
            "    result = method(self, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n",
            "    raise e.with_traceback(filtered_tb) from None\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/ops.py\", line 1973, in _create_c_op\n",
            "    raise ValueError(e.message)\n",
            "ValueError: Exception encountered when calling layer \"conv2d_3\" (type Conv2D).\n",
            "\n",
            "Negative dimension size caused by subtracting 3 from 2 for '{{node conv2d_3/Conv2D}} = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], explicit_paddings=[], padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true](Placeholder, conv2d_3/Conv2D/ReadVariableOp)' with input shapes: [?,3,2,216], [3,3,216,216].\n",
            "\n",
            "Call arguments received by layer \"conv2d_3\" (type Conv2D):\n",
            "  • inputs=tf.Tensor(shape=(None, 3, 2, 216), dtype=float32)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-5b4daa0ac11d>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtuner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstop_early\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Get the optimal hyperparameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mbest_hps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtuner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_best_hyperparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras_tuner/engine/base_tuner.py\u001b[0m in \u001b[0;36msearch\u001b[0;34m(self, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[1;32m    229\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_trial_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_run_and_update_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_trial_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_search_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras_tuner/engine/base_tuner.py\u001b[0m in \u001b[0;36mon_trial_end\u001b[0;34m(self, trial)\u001b[0m\n\u001b[1;32m    333\u001b[0m             \u001b[0mtrial\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mTrial\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m         \"\"\"\n\u001b[0;32m--> 335\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moracle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    336\u001b[0m         \u001b[0;31m# Display needs the updated trial scored by the Oracle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_display\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_trial_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moracle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrial_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras_tuner/engine/oracle.py\u001b[0m in \u001b[0;36mwrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0mLOCKS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moracle\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0mTHREADS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moracle\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mthread_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mret_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mneed_acquire\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m             \u001b[0mTHREADS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moracle\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras_tuner/engine/oracle.py\u001b[0m in \u001b[0;36mend_trial\u001b[0;34m(self, trial)\u001b[0m\n\u001b[1;32m    432\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend_order\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrial_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 434\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_consecutive_failures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras_tuner/engine/oracle.py\u001b[0m in \u001b[0;36m_check_consecutive_failures\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    384\u001b[0m                 \u001b[0mconsecutive_failures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mconsecutive_failures\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_consecutive_failed_trials\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 386\u001b[0;31m                 raise RuntimeError(\n\u001b[0m\u001b[1;32m    387\u001b[0m                     \u001b[0;34m\"Number of consecutive failures excceeded the limit \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m                     \u001b[0;34mf\"of {self.max_consecutive_failed_trials}.\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Number of consecutive failures excceeded the limit of 3.\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/keras_tuner/engine/base_tuner.py\", line 270, in _try_run_and_update_trial\n    self._run_and_update_trial(trial, *fit_args, **fit_kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/keras_tuner/engine/base_tuner.py\", line 235, in _run_and_update_trial\n    results = self.run_trial(trial, *fit_args, **fit_kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/keras_tuner/engine/tuner.py\", line 287, in run_trial\n    obj_value = self._build_and_fit_model(trial, *args, **copied_kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/keras_tuner/engine/tuner.py\", line 213, in _build_and_fit_model\n    model = self._try_build(hp)\n  File \"/usr/local/lib/python3.10/dist-packages/keras_tuner/engine/tuner.py\", line 155, in _try_build\n    model = self._build_hypermodel(hp)\n  File \"/usr/local/lib/python3.10/dist-packages/keras_tuner/engine/tuner.py\", line 146, in _build_hypermodel\n    model = self.hypermodel.build(hp)\n  File \"<ipython-input-18-e06a5bd3019d>\", line 47, in model_builder\n    convBlock(model, hp_block_filter, (hp_block_kernel,hp_block_kernel), (hp_maxpooling,hp_maxpooling))\n  File \"<ipython-input-18-e06a5bd3019d>\", line 34, in convBlock\n    model.add(layers.Conv2D(filter, kernel, activation='relu'))\n  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/trackable/base.py\", line 205, in _method_wrapper\n    result = method(self, *args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n    raise e.with_traceback(filtered_tb) from None\n  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/ops.py\", line 1973, in _create_c_op\n    raise ValueError(e.message)\nValueError: Exception encountered when calling layer \"conv2d_3\" (type Conv2D).\n\nNegative dimension size caused by subtracting 3 from 2 for '{{node conv2d_3/Conv2D}} = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], explicit_paddings=[], padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true](Placeholder, conv2d_3/Conv2D/ReadVariableOp)' with input shapes: [?,3,2,216], [3,3,216,216].\n\nCall arguments received by layer \"conv2d_3\" (type Conv2D):\n  • inputs=tf.Tensor(shape=(None, 3, 2, 216), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Get best number of epochs**"
      ],
      "metadata": {
        "id": "dGvNJpq4HxO8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the model with the optimal hyperparameters and train it on the data for 50 epochs\n",
        "model = tuner.hypermodel.build(best_hps)\n",
        "history = model.fit(images_train, labels_train, epochs=50, validation_split=0.2)\n",
        "\n",
        "val_acc_per_epoch = history.history['val_accuracy']\n",
        "best_epoch = val_acc_per_epoch.index(max(val_acc_per_epoch)) + 1\n",
        "print('Best epoch: %d' % (best_epoch,))"
      ],
      "metadata": {
        "id": "-0GuOwW5HuiQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(images_test, labels_test)"
      ],
      "metadata": {
        "id": "QtxBiU9IIHY8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}